## ğŸš€ DeepSeek-R1-Distill-Qwen-1.5B â€” Full Setup, Quantization & Benchmarking Guide

Welcome! ğŸ‘‹ This guide helps you set up, quantize, serve, and benchmark the **DeepSeek-R1-Distill-Qwen-1.5B** model using `llama.cpp`, `vLLM`, and ROCm tools.

> ğŸ§  Whether you're experimenting, benchmarking, or deploying in production â€” follow this step-by-step!

---

## ğŸ” SSH Access

Login to your cloud machines to get started:

```bash
ssh ubuntu@193.143.78.158
# Inside:
ssh gpu-22

ssh ubuntu@193.143.78.200
ssh gpu-60
````

---

## ğŸ“¦ Download the Model

Use Hugging Face CLI to download the base model:

```bash
huggingface-cli download deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B --local-dir ../DeepSeek-R1-Distill-Qwen-1.5B
```

---

## ğŸ—ï¸ Clone & Build `llama.cpp`

```bash
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
cmake -B build
cmake --build build --config Release
```
> ğŸ”§ To ADD BUILD for AMD GPU's :

```bash
#hip build 
cd /var/lib/ubuntu/vn2/llama.cpp
rm -rf build
mkdir build && cd build

cmake .. \
  -DGGML_HIP=ON \
  -DAMDGPU_TARGETS=gfx942 \
  -DCMAKE_HIP_ARCHITECTURES=gfx942 \
  -DCMAKE_BUILD_TYPE=Release

cmake --build . --config Release -- -j16
```

---

## ğŸ” Convert HF to GGUF Format

Make sure the model is downloaded, then convert it:

```bash
cd llama.cpp
python convert_hf_to_gguf.py ../DeepSeek-R1-Distill-Qwen-1.5B --outfile DeepSeek-R1-Distill-Qwen-1.5B-f16.gguf
python convert_hf_to_gguf.py ../gsm8k_v8_pass1 --outfile Distilled_Qwen1.5B-f16_gsm8k_v8_pass1_qat.gguf
```

---

## âš™ï¸ Add Custom Quantization Logic

Manually copy:

* `quant_config.py`
* `quant_map.py`

into your working directory.

---

## ğŸ”§ Run Layerwise Dynamic Quantization

```bash
chmod +x run_custom_quant_TEST.sh
./run_custom_quant_TEST.sh
```

---

## ğŸ§© Serve Quantized Model (llama.cpp)

> ğŸ¤¡ Use Tmux: Inside one shell, serve the model. In other shell, Benchmark the model using benchmarking scripts.

```bash
HIP_VISIBLE_DEVICES=3 ./llama.cpp/build/bin/llama-server   -m ./DYNAMIC_QUANTIZATION/gguf/DeepSeek-R1-Distill-Qwen-1.5B-gsm8k_v8_qat.gguf   -c 32678   --port 1596   -t 8   -ngl 28
```

>  To apply batching, run the below code [Adjust np based on your preference].

 ```bash
  HIP_VISIBLE_DEVICES=3 ./llama.cpp/build/bin/llama-server   -m ./DYNAMIC_QUANTIZATION/gguf/DeepSeek-R1-Distill-Qwen-1.5B-gsm8k_v8_qat.gguf   -c 32678   --port 1596   -t 8   -ngl 28 -np 8 -cb
 ```

ğŸ“‚ GGUF files:

> âœ¨ Includes Other gguf files for benchmarking

* `DeepSeek-R1-Distill-Qwen-1.5B-mixed-from-json2.gguf`
* `DeepSeek-R1-Distill-Qwen-1.5B-gsm8k_v8_qat.gguf`

---

## ğŸ³ Serve OG Model with Docker + vLLM

> ğŸ¤¡ Use Tmux: Inside one shell, serve the model. In other shell, Benchmark the model using benchmarking scripts.

```bash
docker run -it \
  -e HIP_VISIBLE_DEVICES=1 \
  --network=host \
  --group-add=video \
  --ipc=host \
  --cap-add=SYS_PTRACE \
  --security-opt seccomp=unconfined \
  --device /dev/kfd \
  --device /dev/dri \
  -v /shareddata/dheyo/varunika/benchmarking/DeepSeek-R1-Distill-Qwen-1.5B:/app/model \
  rocm/vllm \
  bash -c 'python -m vllm.entrypoints.openai.api_server --model /app/model --port 1400 --served-model-name deepseek_qwen_distill'
```

---

## ğŸ Serve Unsloth GGUF Models

### ğŸ“¥ Download Models:

```bash
huggingface-cli download unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF DeepSeek-R1-Distill-Qwen-1.5B-Q2_K_L.gguf --local-dir ../DeepSeek-R1-Distill-Qwen-1.5B
huggingface-cli download unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF DeepSeek-R1-Distill-Qwen-1.5B-Q8_0.gguf --local-dir ../DeepSeek-R1-Distill-Qwen-1.5B
huggingface-cli download unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF DeepSeek-R1-Distill-Qwen-1.5B-Q5_K_M.gguf --local-dir ../DeepSeek-R1-Distill-Qwen-1.5B
```

### â–¶ï¸ Serve Commands:

> ğŸ¤¡ Use Tmux: Inside one shell, serve the model. In other shell, Benchmark the model using benchmarking scripts.

```bash
HIP_VISIBLE_DEVICES=1 ./llama.cpp/build/bin/llama-server \
  -m ./DeepSeek-R1-Distill-Qwen-1.5B/DeepSeek-R1-Distill-Qwen-1.5B-Q2_K_L.gguf \
  -c 2048 \
  --port 1700 \
  -t 8 \
  -ngl 28
```

```bash
HIP_VISIBLE_DEVICES=1 ./llama.cpp/build/bin/llama-server \
  -m ../DeepSeek-R1-Distill-Qwen-1.5B/DeepSeek-R1-Distill-Qwen-1.5B-Q8_0.gguf \
  -c 2048 \
  --port 1703 \
  -t 8 \
  -ngl 28
```

```bash
HIP_VISIBLE_DEVICES=1 ./llama.cpp/build/bin/llama-server \
  -m ../DeepSeek-R1-Distill-Qwen-1.5B/DeepSeek-R1-Distill-Qwen-1.5B-Q5_K_M.gguf \
  -c 32678 \
  --port 1805 \
  -t 8 \
  -ngl 28
```

---

## ğŸ“Š Run Benchmarks

```bash
source bench_env/bin/activate
pip install datasets pandas requests
python3 bench.py
```

---

## ğŸ’¾ Download Result Files

### ğŸ–¥ï¸ On Server:

```bash
mv ~/vn2/mmlu_* .
```

### ğŸ“¤ From Server to Local Machine:

```bash
scp ubuntu@193.143.78.158:/shareddata/dheyo/abhilash/vn/mmlu_* .
```

---

## ğŸ§ª TO Run LiveCodeBench (LCB)

```bash
export HIP_VISIBLE_DEVICES=7
VLLM_USE_TRITON_FLASH_ATTN=0 python -m lcb_runner.runner.main \
  --model "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B" \
  --scenario codeexecution \
  --evaluate \
  --release_version release_v1
```

## âš—ï¸ TO RUN Lm_Evaluation_Harness ğŸ™ˆ

> Let's redirect to the working directory and environment, in case setting up from start, follow https://github.com/EleutherAI/lm-evaluation-harness/blob/main/README.md


```bash
cd /shareddata/dheyo/varunika/benchmarking/llmeval/DYNAMIC_QUANTIZATION/lm-evaluation-harness
source lm_eval/bin/activate
```


> ğŸ‘½ï¸ Once redirected, run the command!
> ğŸš§ Change the path and tokenizer based on the model you wish to serve!!

```bash
HIP_VISIBLE_DEVICES=3 lm_eval \
    --model hf \
    --model_args pretrained=/shareddata/dheyo/varunika/benchmarking/llmeval/DYNAMIC_QUANTIZATION/gguf/,gguf_file=DeepSeek-R1-Distill-Qwen-1.5B-gsm8k_v8_qat.gguf,tokenizer=/shareddata/dheyo/varunika/benchmarking/llmeval/gsm8k_v8 \
    --tasks gsm8k \
    --device cuda:0 \
    --batch_size 1
```

> To run it according to our settings and hyperparameters, I created a bash script.
```bash
cd /shareddata/dheyo/varunika/benchmarking/llmeval/DYNAMIC_QUANTIZATION/lm-evaluation-harness
chmod +x run_eval.sh
./run_eval.sh
```
> To change hyperparameters or GPU, edit the bash script.


## ğŸ·ï¸ QUANTIZATION AWARE TRAINING ğŸŒ±


> ğŸ”ï¸ Let's redirect to the working directory and environment, in case setting up from start, follow https://github.com/dheyoai/torchtune/tree/dheyo_fp4
```bash
cd /shareddata/dheyo/varunika/QAT/torchtune/recipes/configs/distilled_qwen2_5/1.5B_qat_full.yaml
```


> ğŸ©¹ This config assumes that you've run the following command before launching:
```bash
tune download deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B --output-dir /shareddata/dheyo/varunika/QAT/torchtune/models/DeepSeek-R1-Distill-Qwen-1.5B
```

> ğŸ§µ To launch on 2 devices, run the following command from root:
```bash
export HIP_VISIBLE_DEVICES=0,1
tune run --nproc_per_node 2 qat_distributed --config recipes/configs/distilled_qwen2_5/1.5B_qat_full.yaml
```


> âš—ï¸ You can add specific overrides through the command line. For example, to override the checkpointer directory while launching training:
```bash
tune run --nproc_per_node 2 full_finetune_distributed --config qwen2_5/1.5B_full checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
```


> âš¡ï¸ Keep changing the no of epochs, lr, batch size


# ğŸ‘· To add support for a new dataset: ğŸ‘·

> ğŸš‘ï¸ change 1: Go to the directory: /shareddata/dheyo/varunika/QAT/torchtune/recipes/configs/distilled_qwen2_5/1.5B_qat_full.yaml ğŸ”ï¸

```bash
dataset:
  _component_: torchtune.datasets.new_dataset_name <---- change here
  packed: False  # True increases speed
  split: train[:95%]
seed: null
shuffle: True

# Validation
run_val_every_n_steps: 100 # Change to an integer to enable validation every N steps
dataset_val:
  _component_: torchtune.datasets.new_dataset_name <---- change here
  split: train[95%:]
batch_size_val: ${batch_size}
```

> âœ… After the above change, go to the directory:

```bash
cd /shareddata/dheyo/varunika/QAT/torchtune/torchtune/datasets
Add a file "_dataset_name.py" ( example :_gsm8k.py )
## Refer to the documentation provided (hard copy) for more details about the code.
Add related functions in _messages.py (/shareddata/dheyo/varunika/QAT/torchtune/torchtune/data/_messages.py) and __init__.py.
```

> âœ¨ To add support for the new model, REFER TO THE DOCUMENTATION !! âœ¨

---

> ğŸ‰ Youâ€™re all set to benchmark and deploy DeepSeek models efficiently!
> ğŸ’¬ For issues or contributions, feel free to open a discussion or PR.

```
